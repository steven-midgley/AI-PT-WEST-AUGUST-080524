{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables.\n",
    "load_dotenv()\n",
    "\n",
    "# Set the model name for our LLMs.\n",
    "GEMINI_MODEL = \"gemini-1.5-flash\"\n",
    "\n",
    "# Store the API key in a variable.\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two very common breeds of cats are:\n",
      "\n",
      "1. **Persian:** Known for their long, luxurious fur and sweet, docile personalities.\n",
      "2. **Siamese:**  Famous for their striking blue almond-shaped eyes and pointed markings. \n",
      "\n",
      "\n",
      "Please provide me with the list of items you'd like to compare for shedding! I need to know what you're referring to in order to tell you which sheds the least. \n",
      "\n",
      "For example, are you asking about:\n",
      "\n",
      "* **Pet breeds?** (e.g., Poodles vs. German Shepherds)\n",
      "* **Types of clothing?** (e.g., Cotton vs. Wool)\n",
      "* **Household materials?** (e.g., Carpet vs. Hardwood floors)\n",
      "\n",
      "Once you give me the list, I can help you figure out which sheds the least! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model.\n",
    "llm = ChatGoogleGenerativeAI(google_api_key=GEMINI_API_KEY, model=GEMINI_MODEL, temperature=0.3)\n",
    "\n",
    "# Define a query as a string.\n",
    "query = \"What are two common breeds of cats?\"\n",
    "\n",
    "# Pass the query to the invoke method, and print the result.\n",
    "result = llm.invoke(query)\n",
    "print(result.content)\n",
    "\n",
    "print()\n",
    "\n",
    "# Define a query as a string.\n",
    "query = \"Which of those shed the least?\"\n",
    "\n",
    "# Pass the query to the invoke method, and print the result.\n",
    "result = llm.invoke(query)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for conversational memory.\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\metin\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:151: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: What are two common breeds of cats?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Two common breeds of cats are the **Siamese** and the **Persian**. \n",
      "\n",
      "* **Siamese cats** are known for their striking blue almond-shaped eyes and pointed markings on their face, ears, paws, and tail. They are also known for their talkative and affectionate personalities. \n",
      "* **Persian cats** are known for their long, luxurious fur and flat faces. They are generally calm and gentle cats, making them popular pets for families. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What are two common breeds of cats?\n",
      "AI: Two common breeds of cats are the **Siamese** and the **Persian**. \n",
      "\n",
      "* **Siamese cats** are known for their striking blue almond-shaped eyes and pointed markings on their face, ears, paws, and tail. They are also known for their talkative and affectionate personalities. \n",
      "* **Persian cats** are known for their long, luxurious fur and flat faces. They are generally calm and gentle cats, making them popular pets for families. \n",
      "\n",
      "Human: Which of those shed the least?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "That's a great question!  While both Siamese and Persian cats shed, **Persian cats are known to shed more** due to their long, thick coats.  \n",
      "\n",
      "Siamese cats have a shorter coat and shed less frequently.  However, all cats shed to some degree, so regular grooming is important for both breeds. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model.\n",
    "llm = ChatGoogleGenerativeAI(google_api_key=GEMINI_API_KEY, model=GEMINI_MODEL, temperature=0.3)\n",
    "\n",
    "# Initialize an object for conversational memory.\n",
    "buffer = ConversationBufferMemory()\n",
    "\n",
    "# Create the chain for conversation, using a ConversationBufferMemory object.\n",
    "conversation = ConversationChain(llm=llm, verbose=True, memory=buffer)\n",
    "\n",
    "# Define a query as a string.\n",
    "query = \"What are two common breeds of cats?\"\n",
    "\n",
    "# Pass the query to the predict method and print the result.\n",
    "result = conversation.predict(input=query)\n",
    "print(result)\n",
    "\n",
    "print()\n",
    "\n",
    "# Define a query as a string.\n",
    "query = \"Which of those shed the least?\"\n",
    "\n",
    "# Pass the query to the predict method, and print the result.\n",
    "result = conversation.predict(input=query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summaries as memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for summary-based memory.\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: What are two common breeds of cats?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Two common breeds of cats are Siamese and Persian. \n",
      "\n",
      "Siamese cats are known for their striking blue almond-shaped eyes and their pointed coloration, where their body is lighter than their face, ears, legs, and tail. They are also known for their vocalizations, which are often described as a \"mew\" or a \"chirp.\" \n",
      "\n",
      "Persian cats are known for their long, luxurious fur and their flat faces. They are often described as being gentle and affectionate.  \n",
      "\n",
      "Are you interested in learning more about these breeds, or would you like to know about other common breeds of cats? \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human asks the AI what two common breeds of cats are. The AI responds that two common breeds of cats are Siamese and Persian. The AI then provides a description of each breed, highlighting their distinctive features and personalities. \n",
      "\n",
      "Human: Which of those shed the least?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "That's a great question!  It's important to consider shedding when choosing a cat breed, as some can be quite furry and leave a lot of hair around the house. \n",
      "\n",
      "Of the two breeds we discussed, **Siamese cats generally shed less than Persians**. This is because Persians have a very thick, long coat that requires regular brushing to prevent mats and tangles. While Siamese cats do shed, their shorter, finer coat tends to be less noticeable. \n",
      "\n",
      "However, it's important to remember that **all cats shed to some degree**. Even breeds considered to be \"low-shedding\" will still lose some hair, especially during seasonal changes. \n",
      "\n",
      "If you're looking for a cat that sheds very little, you might consider exploring other breeds like the **Balinese**, **Russian Blue**, or **Cornish Rex**. These breeds have shorter, finer coats that shed less frequently. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human asks the AI what two common breeds of cats are. The AI responds that two common breeds of cats are Siamese and Persian. The AI then provides a description of each breed, highlighting their distinctive features and personalities. The human then asks which of those breeds sheds the least. The AI explains that Siamese cats generally shed less than Persians due to their shorter, finer coat. However, the AI also emphasizes that all cats shed to some degree and suggests exploring other breeds like the Balinese, Russian Blue, or Cornish Rex for those seeking a cat that sheds very little. \n",
      "\n",
      "Human: Which one tends to weigh more?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "That's a great question!  It's hard to say definitively which breed, Siamese or Persian, tends to weigh more because there's a lot of variation within each breed. \n",
      "\n",
      "* **Siamese cats** typically weigh between 8 and 12 pounds. \n",
      "* **Persian cats** can be a bit heavier, with males often weighing between 10 and 15 pounds. \n",
      "\n",
      "However, it's important to remember that these are just averages. A lot depends on the individual cat's genetics, diet, and activity level.  \n",
      "\n",
      "Do you have any other questions about these breeds or cats in general? ðŸ˜Š \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model for summaries.\n",
    "summary_llm = llm = ChatGoogleGenerativeAI(google_api_key=GEMINI_API_KEY, model=GEMINI_MODEL, temperature=0.0)\n",
    "\n",
    "# Initialize the model for output.\n",
    "chat_llm =  llm = ChatGoogleGenerativeAI(google_api_key=GEMINI_API_KEY, model=GEMINI_MODEL, temperature=0.7)\n",
    "\n",
    "\n",
    "# Initialize an object for conversational memory.\n",
    "buffer = ConversationSummaryMemory(llm=summary_llm)\n",
    "\n",
    "# Create the chain for conversation, using a ConversationBufferMemory object.\n",
    "conversation = ConversationChain(llm=chat_llm, memory=buffer, verbose=True)\n",
    "\n",
    "# Define a query as a string.\n",
    "query = \"What are two common breeds of cats?\"\n",
    "\n",
    "# Pass the query to the predict method, and print the result.\n",
    "result = conversation.predict(input=query)\n",
    "print(result)\n",
    "\n",
    "print()\n",
    "\n",
    "# Define a query as a string.\n",
    "query = \"Which of those shed the least?\"\n",
    "\n",
    "# Pass the query to the predict method, and print the result.\n",
    "result = conversation.predict(input=query)\n",
    "print(result)\n",
    "\n",
    "print()\n",
    "\n",
    "# Define a query as a string.\n",
    "query = \"Which one tends to weigh more?\"\n",
    "\n",
    "# Pass the query to the predict method, and print the result.\n",
    "result = conversation.predict(input=query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity-based Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for entity-based memory.\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Godric': '', 'Maine Coon': '', 'Luna': '', 'Bombay': ''}\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Last line:\n",
      "Human: There are two cats up for adoption: Godric is a Maine Coon, and Luna is a Bombay.\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Bombay': 'Luna is a Bombay.'}\n",
      "\n",
      "Current conversation:\n",
      "Human: There are two cats up for adoption: Godric is a Maine Coon, and Luna is a Bombay.\n",
      "AI: That's great!  I'm ready to help you learn more about Godric and Luna.  Are you interested in knowing more about Maine Coons or Bombay cats? Or, maybe you'd like to know their personalities or what kind of home they might be best suited for?  Just let me know what you'd like to know! \n",
      "\n",
      "Last line:\n",
      "Human: Which of those cats probably has shorter fur?\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Luna, the Bombay, would likely have shorter fur. Bombay cats are known for their sleek, short coats. Maine Coons, on the other hand, are famous for their long, luxurious fur. \n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{}\n",
      "\n",
      "Current conversation:\n",
      "Human: There are two cats up for adoption: Godric is a Maine Coon, and Luna is a Bombay.\n",
      "AI: That's great!  I'm ready to help you learn more about Godric and Luna.  Are you interested in knowing more about Maine Coons or Bombay cats? Or, maybe you'd like to know their personalities or what kind of home they might be best suited for?  Just let me know what you'd like to know! \n",
      "\n",
      "Human: Which of those cats probably has shorter fur?\n",
      "AI: Luna, the Bombay, would likely have shorter fur. Bombay cats are known for their sleek, short coats. Maine Coons, on the other hand, are famous for their long, luxurious fur. \n",
      "\n",
      "Last line:\n",
      "Human: Which cat probably has a calmer temperament?\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "That's a great question!  While both breeds can be sweet and loving,  **Bombay cats are generally known for being more laid-back and mellow**. They often enjoy lounging around and being petted.  Maine Coons, while incredibly friendly, can be more playful and energetic, especially when they're kittens.  They're known for their playful personalities and love to climb and explore.  \n",
      "\n",
      "Do you have any other questions about Godric and Luna?  I'm happy to help you learn more! \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model for entities.\n",
    "entity_llm = ChatGoogleGenerativeAI(google_api_key=GEMINI_API_KEY, model=GEMINI_MODEL, temperature=0.0)\n",
    "\n",
    "\n",
    "# Initialize the model for output.\n",
    "chat_llm = ChatGoogleGenerativeAI(google_api_key=GEMINI_API_KEY, model=GEMINI_MODEL, temperature=0.7)\n",
    "\n",
    "\n",
    "# Initialize an object for conversational memory.\n",
    "buffer = ConversationEntityMemory(llm=entity_llm)\n",
    "\n",
    "# Create the chain for conversation, using a ConversationBufferMemory object.\n",
    "conversation = ConversationChain(\n",
    "    llm=chat_llm, \n",
    "    memory=buffer, \n",
    "    verbose=True, \n",
    "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    ")\n",
    "\n",
    "# Define a query as a string.\n",
    "query = \"There are two cats up for adoption: Godric is a Maine Coon, and Luna is a Bombay.\"\n",
    "\n",
    "# Pass the query to the predict method.\n",
    "result = conversation.predict(input=query)\n",
    "\n",
    "# Define a query as a string.\n",
    "query = \"Which of those cats probably has shorter fur?\"\n",
    "\n",
    "# Pass the query to the predict method, and print the result.\n",
    "result = conversation.predict(input=query)\n",
    "print(result)\n",
    "\n",
    "# Define a query as a string.\n",
    "query = \"Which cat probably has a calmer temperament?\"\n",
    "\n",
    "# Pass the query to the predict method, and print the result.\n",
    "result = conversation.predict(input=query)\n",
    "print(result)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
